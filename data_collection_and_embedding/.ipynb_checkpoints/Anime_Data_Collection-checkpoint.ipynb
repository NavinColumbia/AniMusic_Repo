{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6be1b24-6c81-4831-a9e4-9c278661deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver-manager) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd8b71b-dacf-48a6-bf0e-182958dc43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " offset 1 \n",
      " scraping songs for 'Fullmetal Alchemist: Brotherhood' from https://anidb.net/anime/6107 \n",
      " scraping songs for 'Steins;Gate' from https://anidb.net/anime/7729 \n",
      " scraping songs for 'Shingeki no Kyojin Season 3 Part 2' from https://anidb.net/anime/14444 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/vlljh3455c3g6tcb1l484sqm0000gn/T/ipykernel_86504/644162137.py:323: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(non_empty_batch, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended batch of 45 songs to songs.csv.\n",
      "Total songs saved so far: 45\n",
      "Selenium sessions closed.\n",
      "Total batches processed: 1\n",
      "Total songs saved: 45\n",
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"MAL_CLIENT_ID\")\n",
    "OUTPUT_CSV = \"songs.csv\"\n",
    "BATCH_SIZE = 3\n",
    "SLEEP_INTERVAL = 1\n",
    "START_OFFSET = 1\n",
    "MAX_OFFSET = 4\n",
    "# so start offset to max_offset processed in batches of nbatch_size \n",
    "OUTPUT_MODE = \"append\"\n",
    "\n",
    "\n",
    "class BotDetectionException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    cleaned = re.sub(r'[^\\w\\s]', ' ', text, flags=re.UNICODE)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def check_bot_detection(page_text):\n",
    "    # anidb shows unban me with checkbox after being detected as bot so fot tgat\n",
    "    keywords = [\"unban me\"]\n",
    "    lower_text = page_text.lower()\n",
    "    return any(keyword in lower_text for keyword in keywords)\n",
    "\n",
    "def get_top_anime_batch(batch_size, start_offset):\n",
    "\n",
    "    url = \"https://api.myanimelist.net/v2/anime/ranking\"\n",
    "    anime_list = []\n",
    "    params = {\n",
    "        \"ranking_type\": \"all\",\n",
    "        \"limit\": batch_size,\n",
    "        \"offset\": start_offset,\n",
    "        \"fields\": \"genres,mean,rank,popularity,status,num_episodes,start_date,end_date,studios,source,broadcast,rating,duration,themes\"\n",
    "    }\n",
    "    headers = {\"X-MAL-CLIENT-ID\": CLIENT_ID}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" not in data or not data[\"data\"]:\n",
    "                print(f\"Empty data response for offset {start_offset}.\")\n",
    "                return []\n",
    "            for entry in data[\"data\"]:\n",
    "                node = entry[\"node\"]\n",
    "                anime_data = {\n",
    "                    \"MAL_ID\": node.get(\"id\"),\n",
    "                    \"MAL_Title\": node.get(\"title\"),\n",
    "                    \"MAL_Score\": node.get(\"mean\"),\n",
    "                    \"MAL_Rank\": node.get(\"rank\"),\n",
    "                    \"MAL_Popularity\": node.get(\"popularity\"),\n",
    "                    \"MAL_Status\": node.get(\"status\"),\n",
    "                    \"MAL_Genres\": \", \".join([genre[\"name\"] for genre in node.get(\"genres\", [])]) if node.get(\"genres\") else None,\n",
    "                    \"MAL_Num_Episodes\": node.get(\"num_episodes\"),\n",
    "                    \"MAL_Start_Date\": node.get(\"start_date\"),\n",
    "                    \"MAL_End_Date\": node.get(\"end_date\"),\n",
    "                    \"MAL_Studios\": \", \".join([studio[\"name\"] for studio in node.get(\"studios\", [])]) if node.get(\"studios\") else None,\n",
    "                    \"MAL_Source\": node.get(\"source\"),\n",
    "                    \"MAL_Broadcast\": node.get(\"broadcast\"),\n",
    "                    \"MAL_Rating\": node.get(\"rating\"),\n",
    "                    \"MAL_Duration\": node.get(\"duration\"),\n",
    "                    \"MAL_Themes\": \", \".join([theme[\"name\"] for theme in node.get(\"themes\", [])]) if node.get(\"themes\") else None,\n",
    "                    \"AniDB_Link\": None  # to be filled from MAL page scraping\n",
    "                }\n",
    "                anime_list.append(anime_data)\n",
    "        else:\n",
    "            print(f\"MAL API error at offset {start_offset} (status {response.status_code}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during MAL API call at offset {start_offset}: {e}\")\n",
    "    time.sleep(SLEEP_INTERVAL)\n",
    "    return anime_list\n",
    "\n",
    "def get_anidb_link_from_mal_page(mal_driver, mal_url):\n",
    "    try:\n",
    "        mal_driver.get(mal_url)\n",
    "        WebDriverWait(mal_driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        page_source = mal_driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        mal_title_jp = None\n",
    "        mal_title_en = None\n",
    "        title_container = soup.find(\"div\", itemprop=\"name\")\n",
    "        if title_container:\n",
    "            jp_tag = title_container.find(\"h1\", class_=re.compile(\"title-name\"))\n",
    "            en_tag = title_container.find(\"p\", class_=re.compile(\"title-english\"))\n",
    "            if jp_tag:\n",
    "                mal_title_jp = clean_text(jp_tag.get_text(strip=True))\n",
    "            if en_tag:\n",
    "                mal_title_en = clean_text(en_tag.get_text(strip=True))\n",
    "        if not mal_title_jp and soup.title:\n",
    "            full_title = soup.title.get_text(strip=True)\n",
    "            if \"(\" in full_title:\n",
    "                parts = full_title.split(\" - \")[0]\n",
    "                match = re.match(r'^(.*?)\\s*\\((.*?)\\)$', parts)\n",
    "                if match:\n",
    "                    mal_title_jp = clean_text(match.group(1).strip())\n",
    "                    mal_title_en = clean_text(match.group(2).strip())\n",
    "        \n",
    "\n",
    "        anidb_link = None\n",
    "        resources_header = soup.find(\"h2\", string=lambda text: text and \"Resources\" in text)\n",
    "        if resources_header:\n",
    "            external_links_div = resources_header.find_next_sibling(\"div\", class_=\"external_links\")\n",
    "            if external_links_div:\n",
    "                anidb_a = external_links_div.find(\"a\", href=lambda href: href and \"anidb.net\" in href)\n",
    "                if anidb_a:\n",
    "                    anidb_link = anidb_a.get(\"href\")\n",
    "                    if \"perl-bin/animedb.pl\" in anidb_link and \"aid=\" in anidb_link:\n",
    "                        parsed = urlparse(anidb_link)\n",
    "                        qs = parse_qs(parsed.query)\n",
    "                        aid = qs.get(\"aid\", [None])[0]\n",
    "                        if aid:\n",
    "                            anidb_link = f\"https://anidb.net/anime/{aid}\"\n",
    "        \n",
    "        return {'AniDB_Link': anidb_link, 'MAL_Title_JP': mal_title_jp, 'MAL_Title_EN': mal_title_en}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing MAL page {mal_url}: {e}\")\n",
    "        return {'AniDB_Link': None, 'MAL_Title_JP': None, 'MAL_Title_EN': None}\n",
    "\n",
    "\n",
    "def scrape_anidb_songs(ani_driver, url):\n",
    "\n",
    "    try:\n",
    "        ani_driver.get(url)\n",
    "        WebDriverWait(ani_driver, 15).until(EC.presence_of_element_located((By.ID, \"songlist\")))\n",
    "        page_source = ani_driver.page_source\n",
    "        if check_bot_detection(page_source):\n",
    "            raise BotDetectionException(f\"Bot detection triggered on AniDB page: {url}\")\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout waiting for songlist on {url}.\")\n",
    "        return pd.DataFrame()\n",
    "    except BotDetectionException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Exception accessing AniDB page {url}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    anime_header = soup.find(\"h1\", class_=\"anime\")\n",
    "    anime_name = anime_header.get_text(strip=True).replace(\"Anime: \", \"\") if anime_header else \"Unknown\"\n",
    "    \n",
    "    anime_name = clean_text(anime_name)\n",
    "    \n",
    "    tags = []\n",
    "    tags_row = soup.find(\"tr\", class_=lambda c: c and \"tags\" in c)\n",
    "    if not tags_row:\n",
    "        tags_row = soup.find(\"tr\", lambda tag: tag.name==\"tr\" and tag.find(\"th\") and \"Tags\" in tag.find(\"th\").get_text())\n",
    "    if tags_row:\n",
    "        tag_cells = tags_row.find_all(\"span\", class_=\"tagname\")\n",
    "        tags = [clean_text(tag.get_text(strip=True)) for tag in tag_cells]\n",
    "    \n",
    "    table = soup.find(\"table\", id=\"songlist\")\n",
    "    if table is None:\n",
    "        print(f\"Song table not found for {url}.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    tbody = table.find(\"tbody\")\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    songs = []\n",
    "    current_relation = \"\"\n",
    "    song_map = {}\n",
    "    \n",
    "    for row in rows:\n",
    "        rel_cell = row.find(\"td\", class_=\"reltype\")\n",
    "        if rel_cell:\n",
    "            current_relation = clean_text(rel_cell.get_text(strip=True))\n",
    "        \n",
    "        song_cell = row.find(\"td\", class_=\"name song\")\n",
    "        if song_cell:\n",
    "            song_name = clean_text(song_cell.get_text(strip=True))\n",
    "            song_url = song_cell.find(\"a\").get(\"href\", \"\") if song_cell.find(\"a\") else \"\"\n",
    "            song_data = {\n",
    "                \"relation\": current_relation,\n",
    "                \"song\": song_name,\n",
    "                \"song_url\": song_url,\n",
    "                \"episodes\": \"\",\n",
    "                \"rating\": None,\n",
    "                \"vocals\": \"\",\n",
    "                \"lyrics\": \"\",\n",
    "                \"composition\": \"\",\n",
    "                \"arrangement\": \"\",\n",
    "                \"chorus\": \"\",\n",
    "                \"tags\": \", \".join(tags),\n",
    "                \"anime\": anime_name\n",
    "            }\n",
    "            eprange = row.find(\"td\", class_=\"eprange\")\n",
    "            song_data[\"episodes\"] = clean_text(eprange.get_text(strip=True)) if eprange else \"\"\n",
    "            rating = row.find(\"td\", class_=\"rating\")\n",
    "            if rating:\n",
    "                rating_text = rating.get_text(strip=True)\n",
    "                m = re.match(r\"([\\d\\.]+)\", rating_text)\n",
    "                song_data[\"rating\"] = float(m.group(1)) if m else None\n",
    "            songs.append(song_data)\n",
    "            song_map[song_name] = song_data\n",
    "        \n",
    "        credit_cell = row.find(\"td\", class_=\"credit\")\n",
    "        creator_cell = row.find(\"td\", class_=\"name creator\")\n",
    "        if credit_cell and creator_cell:\n",
    "            credit_type = clean_text(re.sub(r'\\s*\\(.*?\\)', '', credit_cell.get_text(strip=True)))\n",
    "            creators = \", \".join([clean_text(a.get_text(strip=True)) for a in creator_cell.find_all(\"a\")])\n",
    "            if song_name in song_map:\n",
    "                if \"Vocals\" in credit_type:\n",
    "                    song_map[song_name][\"vocals\"] = creators\n",
    "                elif \"Lyrics\" in credit_type:\n",
    "                    song_map[song_name][\"lyrics\"] = creators\n",
    "                elif \"Music Composition\" in credit_type:\n",
    "                    song_map[song_name][\"composition\"] = creators\n",
    "                elif \"Music Arrangement\" in credit_type:\n",
    "                    song_map[song_name][\"arrangement\"] = creators\n",
    "                elif \"Chorus\" in credit_type:\n",
    "                    song_map[song_name][\"chorus\"] = creators\n",
    "                    \n",
    "    return pd.DataFrame(songs)\n",
    "\n",
    "def append_to_csv(df, filename, write_header=False):\n",
    "    mode = \"w\" if write_header else \"a\"\n",
    "    df.to_csv(filename, mode=mode, index=False, header=write_header)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if OUTPUT_MODE.lower() == \"append\" and os.path.exists(OUTPUT_CSV):\n",
    "        first_batch = False\n",
    "    else:\n",
    "        first_batch = True\n",
    "\n",
    "    offset = START_OFFSET\n",
    "    total_batches_processed = 0\n",
    "    total_songs_count = 0\n",
    "\n",
    "    mal_options = webdriver.ChromeOptions()\n",
    "    mal_options.add_argument(\"--headless\")\n",
    "    ani_options = webdriver.ChromeOptions()\n",
    "    ani_options.add_argument(\"--headless\")\n",
    "    try:\n",
    "        mal_driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=mal_options)\n",
    "        ani_driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=ani_options)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Selenium: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        while offset < MAX_OFFSET:\n",
    "            print(f\"\\n offset {offset} \")\n",
    "            batch_anime = get_top_anime_batch(BATCH_SIZE, offset)\n",
    "            if not batch_anime:\n",
    "                print(\" no more mal \")\n",
    "                break\n",
    "\n",
    "            batch_df = pd.DataFrame(batch_anime)\n",
    "            enriched_records = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                mal_url = f\"https://myanimelist.net/anime/{row['MAL_ID']}\"\n",
    "                try:\n",
    "                    result = get_anidb_link_from_mal_page(mal_driver, mal_url)\n",
    "                    row[\"AniDB_Link\"] = result.get(\"AniDB_Link\")\n",
    "                    row[\"MAL_Title_JP\"] = result.get(\"MAL_Title_JP\")\n",
    "                    row[\"MAL_Title_EN\"] = result.get(\"MAL_Title_EN\")\n",
    "                    enriched_records.append(row)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing MAL page for anime ID {row['MAL_ID']}: {e}. Skipping.\")\n",
    "                    continue\n",
    "                time.sleep(SLEEP_INTERVAL)\n",
    "            enriched_df = pd.DataFrame(enriched_records)\n",
    "\n",
    "            batch_songs = []\n",
    "            for idx, row in enriched_df.iterrows():\n",
    "                title_api = row[\"MAL_Title\"]\n",
    "                anidb_link = row.get(\"AniDB_Link\")\n",
    "                if not anidb_link:\n",
    "                    print(f\" '{title_api}'. anidb error .\")\n",
    "                    continue\n",
    "                print(f\" scraping songs for '{title_api}' from {anidb_link} \")\n",
    "                \n",
    "                retry_attempts = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        songs_df = scrape_anidb_songs(ani_driver, anidb_link)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\" Error  '{title_api}': {e}.\")\n",
    "                        songs_df = pd.DataFrame()\n",
    "                        break\n",
    "\n",
    "                if songs_df.empty:\n",
    "                    print(f\" No songs for '{title_api}'.\")\n",
    "                else:\n",
    "                    for col in [\"MAL_ID\", \"MAL_Title\", \"MAL_Title_JP\", \"MAL_Title_EN\",\n",
    "                                \"MAL_Score\", \"MAL_Rank\", \"MAL_Popularity\", \"MAL_Status\",\n",
    "                                \"MAL_Genres\", \"MAL_Num_Episodes\", \"MAL_Start_Date\", \"MAL_End_Date\",\n",
    "                                \"MAL_Studios\", \"MAL_Source\", \"MAL_Broadcast\", \"MAL_Rating\",\n",
    "                                \"MAL_Duration\", \"MAL_Themes\"]:\n",
    "                        songs_df[col] = row.get(col, None)\n",
    "                    songs_df[\"AniDB_Link\"] = anidb_link\n",
    "                    match = re.search(r'/anime/(\\d+)', anidb_link)\n",
    "                    songs_df[\"AniDB_ID\"] = match.group(1) if match else None\n",
    "                    batch_songs.append(songs_df)\n",
    "                time.sleep(SLEEP_INTERVAL)\n",
    "            \n",
    "            non_empty_batch = [df for df in batch_songs if not df.empty]\n",
    "            if non_empty_batch:\n",
    "                combined_df = pd.concat(non_empty_batch, ignore_index=True)\n",
    "                append_to_csv(combined_df, OUTPUT_CSV, write_header=first_batch)\n",
    "                total_songs_count += len(combined_df)\n",
    "                print(f\"Appended batch of {len(combined_df)} songs to {OUTPUT_CSV}.\")\n",
    "                print(f\"Total songs saved so far: {total_songs_count}\")\n",
    "                first_batch = False\n",
    "            else:\n",
    "                print(\" No song data this batch.\")\n",
    "            \n",
    "            offset += BATCH_SIZE\n",
    "            total_batches_processed += 1\n",
    "\n",
    "    except SystemExit as se:\n",
    "        print(se)\n",
    "    except Exception as e:\n",
    "        print(f\" error : {e}\")\n",
    "    finally:\n",
    "        mal_driver.quit()\n",
    "        ani_driver.quit()\n",
    "        print(\"Selenium sessions closed.\")\n",
    "        print(f\"Total batches processed: {total_batches_processed}\")\n",
    "        print(f\"Total songs saved: {total_songs_count}\")\n",
    "        print(\"Processing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0e7a582-d837-4b80-ae86-712f93a3df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in MAL_Title_EN: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./songs.csv\")\n",
    "\n",
    "unique_titles_count = df['MAL_Title'].nunique()\n",
    "\n",
    "print(f\"Number of unique values in MAL_Title_EN: {unique_titles_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "605bf6a2-b464-4f4a-8c16-db54536e0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('songs.csv')\n",
    "\n",
    "\n",
    "df_unique = df.drop_duplicates(subset=['MAL_Title', 'song_url', 'song'])\n",
    "\n",
    "df_unique.to_csv('songs.csv', index=False)\n",
    "\n",
    "df.to_csv(\"songs.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
